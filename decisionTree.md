# 决策树
what?<br>
决策树是一个分类与回归模型，基于特征对实例进行分类，对特征空间和类空间进行划分。
<br>
用决策树分类，从根节点开始，对实例的某一特征进行测试，根据测试结果，将实例划分到其子节点，此时，每一个节点对应特征的一个取值，递归上述过程，直到到达叶子节点，最好将实例分配到叶子节点的类中。

<h2>特征选择</h2>
决策树生成过程需要将实例的某个特征进行测试划分，那么选择哪个特征呢?<br>

**信息熵** <br>
信息熵定义为信息的混乱程度，即越大，信息越混乱。
随机变量X的熵定义为
$
H(X) = -\sum_{i=1}^n p_i log p_i \tag{1}
$
其中,随机变量X有n个取值
$
P(X = x_i) = p_i \tag{2}
$
从1式子中可以看到X的熵与X无关，只与X的概率分布有关。因此X的熵也可以记为$H(p)$ <br>
条件熵$H(Y|X)$ 表示在X确定的情况下，随机变量Y的不确定性,定义为
$
H(Y|X) = \sum_{i=1}^n p_i H(Y| X = x_i) \tag{3}
$
表示在x确定的情况下Y的条件概率熵对X的数学期望
其中
$
P(X = x_i) = p_i  \quad i = 1,2,...,n \tag{4}
$
熵和条件熵可由数据估计得到，称为经验熵和条件经验熵<br>

**信息增益**<br>
特征A对数据集D的信息增益定义为
$
g(D,A) = H(D) - H(D|A) \tag{5}
$
即利用特征A对数据集D进行划分后，熵的减少程度。<br>

**信息增益算法** <br>
设训练数据集为D，$|D|$ ,表示样本个数，有K个类别$C_K,k=1,2,...,K$,$|C_K|$表示属于类$C_K$的样本大小。设特征A有n个取值，${a_1,a_2,...,a_n}$,根据A的取值将D划分为$D_1,D_2,...,D_n$,子集$D_i$中属于类 $C_K$的记为$D_{ik}$ <br>
计算特征A对训练数据集D信息增益
$
H(D) = -\sum_{i=1}^K p_i logp_i = -\sum_{i=1}^K {|D_i| \over |D|} log {|D_i| \over |D|} \tag{6}
$
计算条件熵
$
H(D,A)=\sum_{i=1}^n p_iH(D|A=a_i) = \sum_{i=1}^n {|D_i| \over |D|}H(D|A=a_i) \\=  \sum_{i=1}^n {|D_i| \over |D|} -\sum_{k=1}^K P(D=c_k|A =a_i) logP(D=c_k|A =a_i) \\= -\sum_{i=1}^n {|D_i| \over |D|} \sum_{k=1}^K{|D_{ik}| \over |D_i|}log{|D_{ik}| \over |D_i|} \tag{7}
$
计算信息增益
$
g(D,A) = H(D) - H(D,A) \tag{8}
$

根据上面公式6，7，8可计算任意特征对数据集信息增益，选取信息增益最大的特征对数据集进行划分


<h2>决策树的生成</h2>
决策树可由不同算法生成







