# K 近邻法
what?<br>
k近邻是一种分类与回归办法，通过其最近的K个实例的类别，根据多数表决的方式进行预测。输入实例的特征向量，输出实例的类别。
k近邻不进行显示学习
<br>


how?<br>
因此,决定模型的有三个要素，包括k值选择，距离度量方法，分类决策规则。<br>
**距离度量方法**
n维向量空间的点，$x_i = (x_i^{(1)},x_i^{(2)},...,x_i^{(n)})$ 和$x_j =(x_j^{(1)},x_j^{(2)},...,x_j^{(n)}) $
$
L_p = (\sum_{i=1}^N |x_i^{(l)}  - x_j^{(l)}|^p)^{1 \over p}
$
当p取不同值时，会有不同的定义
<br>
**分类决策规则**
多数表决规则，如果分类损失函数是0-1损失函数，那么分类函数为
$
R^n \rightarrow \{c_1,c_2,..., c_n\}
$
误分类率则为
$
P(Y \neq f(X)) = 1 - P(Y = f(X))
$

对于给定实例$x\in X$,其最近的K个实例点构成$N_k(x)$,如果涵盖$N_k(x)$的区域是$c_j$,则误分类率为
$
{1 \over k}\sum_{x_i \in N_k(x)} I(y_i \neq c_j) = 1 - {1 \over k} \sum I(y_i = c_j)
$
要是误分类率最低，就要${1 \over k} \sum_{x_i \in N_k(x)} I(y_i = c_j)$ 最大,所以多数表决等价于禁言风险最小化